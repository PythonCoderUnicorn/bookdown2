[{"path":"index.html","id":"about","chapter":"1 About","heading":"1 About","text":"book project trying condense various RLadies meetings, videos, lectures talks single book. goal super ambitious one complete.\n","code":""},{"path":"index.html","id":"usage","chapter":"1 About","heading":"1.1 Usage","text":"book chapter topics covered RLadies chapters, code github repository. idea chapter dedicated topic despite RLadies chapter presented .","code":""},{"path":"index.html","id":"rladies-book","chapter":"1 About","heading":"1.2 RLadies book","text":"book idea much RLadies knowledge exists Github repos, meetups YouTube videos one readable format. Due global reach R chapters redundancy. book focus English presentations (now) someone else comes along add project.hope volume knowledge read shared, along additions keep copious RLadies meetups.","code":""},{"path":"hello-rladies.html","id":"hello-rladies","chapter":"2 Hello RLadies","heading":"2 Hello RLadies","text":"chapters RLadies condensed RLadies Global chapter try showcase topic many RLadies chapters possible without redundancy.","code":""},{"path":"hello-rladies.html","id":"start-here","chapter":"2 Hello RLadies","heading":"2.1 StaRt here","text":"section starting RStudio using R. starter code RLadies Freiburg repo. Starting using base R entails use $ dollar sign, used grab specific column dataset.dataset use library called datasets R package contains various datasets used learning needs. Using either simple .R file Rmd file {r} code chunk, enter code basic descriptive statistics.","code":"\n#--- step one is to install the library package\n# install.packages('datasets')    # comment out to run\nlibrary(datasets)                 # load library\n\n# the specific dataset we want is called ChickWeight\ndata(\"ChickWeight\")               # function to pull out specific dataset\n\nhead(ChickWeight)                 # see first 6 rows of dataset\n#>   weight Time Chick Diet\n#> 1     42    0     1    1\n#> 2     51    2     1    1\n#> 3     59    4     1    1\n#> 4     64    6     1    1\n#> 5     76    8     1    1\n#> 6     93   10     1    1"},{"path":"hello-rladies.html","id":"descriptive-stats","chapter":"2 Hello RLadies","heading":"2.2 Descriptive Stats","text":"simple descriptive statistics R using base R functions like mean(), std() max(), among others. ChickWeight dataset, want find mean weight.Now want create new variable deviation mean weight.","code":"\n# find the mean weight. \n# need to use the $ to pull the data from column 'weight'\n\n# - method 1:\n# use the function mean with our dataset$column\nmean(ChickWeight$weight)  \n#> [1] 121.8183\n\n# - method 2:\n# make a variable to store our column data\nchik_wt = ChickWeight$weight\n\navg_chick_wt = mean(chik_wt) # save the mean chick weight as variable\navg_chick_wt\n#> [1] 121.8183\n# create new column.\n# dataset$NEW_COLUMN_NAME \n\nChickWeight$Deviation =  ChickWeight$weight - avg_chick_wt\nChickWeight$Deviation[1:10]\n#>  [1] -79.818339 -70.818339 -62.818339 -57.818339 -45.818339\n#>  [6] -28.818339 -15.818339   3.181661  27.181661  49.181661"},{"path":"hello-rladies.html","id":"for-loop","chapter":"2 Hello RLadies","heading":"2.3 For Loop","text":"can create loop make ChickWeight data 2 categories using ifelse statement make categories: ‘Normal’ ‘Large.’","code":"\n#  for loop to make categories Normal and Large based on weight\nfor (x in 1:nrow(ChickWeight)) {\n  ChickWeight$Large = ifelse(ChickWeight$Deviation < 100, \"Normal\", \"Large\")\n}"},{"path":"hello-rladies.html","id":"data-visual","chapter":"2 Hello RLadies","heading":"2.4 Data Visual","text":"can use base R plotting function plot() plot data.","code":"\nplot(ChickWeight$weight)\nplot(ChickWeight$Diet, ChickWeight$weight)\n# histogram\nhist(ChickWeight$Deviation)"},{"path":"tidyverse.html","id":"tidyverse","chapter":"3 Tidyverse","heading":"3 Tidyverse","text":"tidyverse book called R Data Science Hadley Wickham & Garrett Grolemund (available online). section discuss functions library useful know.","code":""},{"path":"tidyverse.html","id":"tidy-data","chapter":"3 Tidyverse","heading":"3.1 Tidy Data","text":"Data Transformation Introduction Tidyverse library. section RLadies Freiburg Divya Seernani.\nTidy data data data values column (variable) row (observation). paradigm:\nimport -> tidy -> [Transform <-> Visualize <-> Model] -> Communicate","code":""},{"path":"tidyverse.html","id":"dplyr-library","chapter":"3 Tidyverse","heading":"3.1.1 dplyr library","text":"Inside tidyverse library dplyr library helpful creating new variables, renaming reordering observations, selecting specific observations calculating summary statistics among many functions.dataset section Indian Census (2011), dataset available . Remember click ‘raw’ data format first load data, see white background black text CSV data, copy url link order read data.tidyverse library brings along libraries load tidyverse bunch warnings console, concern , basically telling one library hides another function use. turn warnings Rmd file, inside code chunk {r} set warning false {r, warning= FALSE, message= FALSE}.","code":"\n# step 1 - install tidyverse library\n# install.packages('tidyverse')  # comment-out to run \nlibrary(tidyverse)\n\n# read in the data\nindia_census = read.csv('https://raw.githubusercontent.com/nishusharma1608/India-Census-2011-Analysis/master/india-districts-census-2011.csv')"},{"path":"tidyverse.html","id":"filter","chapter":"3 Tidyverse","heading":"3.1.1.1 filter","text":"now want filter data Maharashtra Gajarat, call variable WestStates.","code":"\nWestStates = filter(india_census, state.name == 'Maharashtra' | State.name =='Gajarat') "},{"path":"tidyverse.html","id":"arrange","chapter":"3 Tidyverse","heading":"3.1.1.2 arrange","text":"Arrange west states descending order (highest lowest) agricultural workers.","code":"\nAgriDist<- arrange(WestStates, Agricultural_Workers)\nAgriDistDes<- arrange(WestStates, desc(Agricultural_Workers))"},{"path":"tidyverse.html","id":"mutate","chapter":"3 Tidyverse","heading":"3.1.1.3 mutate","text":"Using mutate function calculate percentage computers & internet per households.","code":"\nindia_census = mutate(india_census,\n                      Percent_Internet = Households_with_Internet / Households * 100,\n                      Percent_Computer = Households_with_Computer / Households * 100\n                      )"},{"path":"tidyverse.html","id":"select","chapter":"3 Tidyverse","heading":"3.1.1.4 select","text":"Now percent homes internet, let us select data households latrines bathing facilities.Visualize data library ggplot, comes attached (loaded) tidyverse, good load anyway.Summary literacy data stateAn exercise example, literacy toilets.","code":"\nModernHomes = select(india_census,\n                     State.name, District.name, Households, Having_bathing_facility_Total_Households,\n                  Having_latrine_facility_within_the_premises_Total_Households)\n\n# and standardize the results\nModernHomes2 = mutate(ModernHomes, PercentToilet = Having_latrine_facility_within_the_premises_Total_Households / Households * 100,\n                     PercentBath = Having_bathing_facility_Total_Households / Households * 100)\nlibrary(ggplot2)\n\nModernHomesPlot <- ggplot(ModernHomes2, aes(x=PercentToilet, y=PercentBath)) + \n  geom_point(aes(col=State.name, size=Households)) + \n  geom_smooth(method=\"glm\", se=T) + \n  labs(subtitle=\"Toilets and Baths\", \n       y=\"Percentage with Bath\", \n       x=\"Percentage with Toilet\", \n       title=\"Scatterplot\", \n       caption = \"Indian Census 2011\")\n\nplot(ModernHomesPlot)\n#> `geom_smooth()` using formula 'y ~ x'\nsummarise(india_census, Percent_Literate = mean(Literate/Population*100) )\n#>   Percent_Literate\n#> 1          62.4632\n\nLiteracy = group_by(india_census, State.name)\n\nLiteracyByState = summarise(Literacy, PercentLiterate = mean(Literate/Population*100) )LiteracyHygiene <- select(india.districts.census.2011, State.name, District.name, Literate, Households, Having_latrine_facility_within_the_premises_Total_Households)\nLiteracyHygiene_Calculated<-mutate(LiteracyHygiene, PercentToilet = Having_latrine_facility_within_the_premises_Total_Households / Households * 100,\n                     AverageLiterate = Literate/Households)\n\nLiteracyHygienePlot <- ggplot(LiteracyHygiene_Calculated, aes(x=PercentToilet, y=AverageLiterate)) + \n  geom_point(aes(col=State.name, size=Households)) + \n  geom_smooth(method=\"glm\", se=T) + \n  labs(subtitle=\"Toilets and Baths\", \n       y=\"Average Literate People per Household\", \n       x=\"Percentage with Toilet\", \n       title=\"Scatterplot\", \n       caption = \"Indian Census 2011\")\n\nplot(LiteracyHygienePlot)\n\nlibrary(plotly)\nggplotly(LiteracyHygienePlot)\n"},{"path":"geo-data.html","id":"geo-data","chapter":"4 Geo Data","heading":"4 Geo Data","text":"section geographical data visualization.","code":""},{"path":"geo-data.html","id":"raster-and-vectors","chapter":"4 Geo Data","heading":"4.1 raster and vectors","text":"tutorial RLadies Freiburg Elisa Schneider.\ndifferent ways plot maps R. packages use depend aim format data. Sometimes, geographical data can simply .txt .csv file, one column latitude one longitude variables measurements corresponding location following columns. Yo can also shape files, special files geographical information. files may contain points, lines polygons. usually .shp files. Finally can raster files something similar pictures .png files files pixels (least) value associated pixel. difference .png file shape file shape-file associated coordinates locate pixels specific surface world. format usually .geotiff just .tiff.meetup cover basis working formats. starting point. lot .","code":""},{"path":"geo-data.html","id":"simplest-example","chapter":"4 Geo Data","heading":"4.1.1 Simplest example","text":"can just use geom_polygon library ggplot create first figure. Let´s suppose know latitude longitude nodes polygon. can plot ?","code":"require(ggplot2)\nfunny <- data.frame(lat = c(62, 55, 48, 48, 62), long = c(20,10.5,20,10,10)) # from top right \n#following the clock\nggplot(funny, aes(x = long, y = lat)) + # we specify the data\n  geom_polygon(fill=\"green\") + # we plot it\n  geom_point(aes(x=13.25, y=50.5), color=\"violet\", size=18) # we can also add points this was"},{"path":"geo-data.html","id":"information-already-availabe-in-r","chapter":"4 Geo Data","heading":"4.1.2 Information already availabe in R","text":"geometry polygons countries world already loaded R. can access info using function map_data package ggplot2.plot one subset countries?need get data geometry countries. use library maps ggplot2.Themes ggplot ggplot2 display non-data components controlled theme system. theme system ggplot2 allows manipulation titles, labels, legends, grid lines backgrounds. various build-themes available already -around consistent style. common one plotting data theme_bw() theme_map(). use theme_map() give plot appearance map. use theme_map() need load package ggthemes().","code":"countries <- c(\"Germany\", \"France\")\n# Get the data\nsome.maps <- map_data(\"world\", region = countries) # This function retrieves the data. The data we get is a df with lat and long of each node. \nhead(some.maps)\nrequire(dplyr)\nrequire(tidyr)\n# Mean of lat and long to then write the labels \nlab.data <- some.maps %>%\n  group_by(region) %>%\n  summarise(long = mean(long), lat = mean(lat))\nlab.datarequire(ggthemes)\nfra_and_ger <- ggplot(some.maps, aes(x = long, y = lat)) +\n  geom_polygon(aes( group = group, fill = region))+ #plot polygon, use group to group all the nodes of the same country together. \n  geom_text(aes(label = region), data = lab.data,  size = 3, hjust = 0.5)\n#using theme_map\nfra_and_ger + #print labels\n  theme_map() +\n  theme(legend.position = \"none\")\n#Using theme bw\nfra_and_ger + #print labels\n  theme_bw() +\n  theme(legend.position = \"none\")+\n  xlab(\"Longitude\") + ylab(\"Latitude\")\n "},{"path":"geo-data.html","id":"the-world","chapter":"4 Geo Data","heading":"4.1.3 the world","text":"couple countries whole world: Plotting simple nice maps worldggplot2 becoming standard R graphs. However, handle spatial data specifically. Handling spatial objects R relies Spatial classes defined package sp sf. ggplot2 allows use spatial data package sf. sf elements can added layers graph. combination ggplot2 sf enables create maps, using grammar graphics,incorporation geographical info.","code":""},{"path":"geo-data.html","id":"we-can-just-create-a-map-from-the-world.","chapter":"4 Geo Data","heading":"4.1.3.1 1. We can just create a map from the world.","text":"using library ggplot2.function ne_countries also retrieves info countries. now get just nodes coordinates sf object contains much info.can plot geographical object using ggplot2 geometry geom_sf","code":"require(rnaturalearth)\nrequire(rnaturalearthdata)\nrequire(sf)\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\") # this is another function to get polygons of countries. \nclass(world)\ndim(world)\nhead(world[, 1:5])plot_w1 <- ggplot(data = world) +\n    theme_bw()+ \n    geom_sf() + \n    xlab(\"Longitude\") + ylab(\"Latitude\") + \n    ggtitle(\"World map\", subtitle = paste0(\"(\", dim(world)[1], \" countries)\"))\nplot_w1"},{"path":"geo-data.html","id":"we-can-change-the-colors-add-text","chapter":"4 Geo Data","heading":"4.1.3.2 2. We can change the colors, add text …","text":"However nice modern-art-style polygon looks different map compared first plot. idea happening?","code":"plot_w1 +\n   geom_sf(color=\"blue\", fill=\"black\") +\n  theme(panel.background = element_rect(fill = \"black\"))+# Modify the theme to change the background \n  #Where was the funny polygon located?\n  geom_polygon(data=funny, aes(x = long, y = lat), color=\"green\", fill=\"green\") + # we plot it\n  # Add points using lat-lon information\n  geom_point(aes(x=13.25, y=50.5), color=\"violet\", size=2)"},{"path":"geo-data.html","id":"coordinate-reference-systems-crs.","chapter":"4 Geo Data","heading":"4.1.3.3 Coordinate reference systems (crs).","text":"Going rear world (earth) simplifyed model (map)","code":""},{"path":"geo-data.html","id":"after-choosing-elipsoid-and-datum-we-have-to-project-from-3d-to-2d.","chapter":"4 Geo Data","heading":"4.1.3.4 After choosing elipsoid and datum we have to project from 3D to 2D.","text":"can get feeling Mercator projection distorts worldview link.Google many apps use unprojected coordinates. coordinates unprojected, degrees give position sphere 2D surface. , still need ellipsoid datum make clear reference system using. information coded EPSG code.\ncommon coordinate reference system crs (used Google apps) EPGS: 4326. lat Lon coordinates info, likely EPSG 4326, uses datum WGS84.get taste:\nEPSG: 4326\nCoordinate Systems WorldwideTo find one used region interest:Argentina\nGermany\nEurope","code":""},{"path":"geo-data.html","id":"make-choropleth-map","chapter":"4 Geo Data","heading":"4.2 Make Choropleth Map","text":"Chorophlet Map? “choropleth map thematic map areas shaded patterned proportion measurement statistical variable displayed map, population density per-capita income.”(Wikipedia)","code":""},{"path":"geo-data.html","id":"load-data","chapter":"4 Geo Data","heading":"4.2.0.1 1. Load Data","text":"data obtained link, lot cool data-sets available free.","code":"require(readr) # to use the function read_csv()\nlife.exp <- read_csv(\"data/LifeExp.csv\")"},{"path":"geo-data.html","id":"tidy-the-data-set","chapter":"4 Geo Data","heading":"4.2.0.2 2. Tidy the data set","text":"one column country another life expectancy","code":"life_exp <- life.exp %>%\n  filter(year == 2016 & sex == \"Both sexes\") %>%  # Keep data for 2016 and for both sex\n  dplyr::select(country, value) %>%                      # Select the two columns of interest\n  rename(name = country, lifeExp = value) %>%   # Rename columns\n  #We have a very common proble when using different sources, the name of the countries not allways match\n  # Replace \"United States of America\" by USA in the region column\n  mutate(\n    name = ifelse(name == \"United States of America\", \"United States\", name), \n    name = ifelse(name == \"Russian Federation\", \"Russia\", name)\n    )"},{"path":"geo-data.html","id":"get-data-of-the-polygons","chapter":"4 Geo Data","heading":"4.2.0.3 3. Get data of the polygons","text":"country merge map life expectancy data","code":"#attribute names\ncolnames(world)\nlife_exp2 <- left_join(world, life_exp, by = \"name\")"},{"path":"geo-data.html","id":"plot-with-ggpoligon","chapter":"4 Geo Data","heading":"4.2.0.3.1 4. Plot with ggpoligon","text":"Clearly information countries regarding life expectancy /problems non-matching country names. call left_join keep table countries left table.","code":"life_exp_map <- ggplot(data = life_exp2) +\n    geom_sf(aes(fill = lifeExp )) +\n    scale_fill_viridis_c(option = \"plasma\", trans = \"sqrt\") # this allows you to choose different colour scale\nlife_exp_map"},{"path":"geo-data.html","id":"add-points","chapter":"4 Geo Data","heading":"4.2.0.4 5. Add points","text":"can also add points plot. example add capitals. Points samples obtained, etc. can also control size color points using another variable. example, plot sample points size number observations. plot one point per city size number inhabitants…son forth.","code":"country_capitals <- read_csv(\"data/country-capitals.csv\")\nsouth_america_capitals <- country_capitals %>% dplyr::filter(ContinentName == \"South America\")\nsouth_america_capitals$CapitalLatitude <- as.numeric(south_america_capitals$CapitalLatitude)\nrequire(ggrepel)\nlife_exp_map +\n  geom_point(data=south_america_capitals, \n             aes(x=CapitalLongitude, y=CapitalLatitude),\n             alpha=0.5)+\n  geom_text_repel(data=south_america_capitals, # geom_text_repel avoidsoverlapping\n            aes( x=CapitalLongitude, y=CapitalLatitude, label=CapitalName), \n            hjust=0, vjust=0, size= 3)+ \n  theme_bw()\n  "},{"path":"geo-data.html","id":"add-scale-and-north","chapter":"4 Geo Data","heading":"4.2.0.5 6. Add scale and north","text":"","code":"require(ggspatial)\nlife_exp_map + \n  annotation_scale(location = \"bl\", width_hint = 0.5) + # scale\n    annotation_north_arrow(location = \"bl\", which_north = \"true\", #north arrow\n        pad_x = unit(0.75, \"in\"), pad_y = unit(0.5, \"in\"),\n        style = north_arrow_fancy_orienteering) +\n    coord_sf(xlim = c(-20.15, 50.12), ylim = c(35.65, -45)) #crop"},{"path":"geo-data.html","id":"rasters-and-polygons","chapter":"4 Geo Data","heading":"4.3 Rasters and polygons","text":"Usually spatial data comes raster polygon format. usually shape files onformation sampling sites, neighborhoods, streets, rivers, hospitals, surveys, etc. saved .shp files.\nlat-Lon data can transform sf object .\ncan also get raster files climate raster files remote sensing data, land use, etc. times want plot info together (raster + one shape-files).libraries allow us work spatial data using R.rgdal library allows us read write geospatial data R. library just translates already existing library gdal R. link gdalproject link.rgdal library allows us read write geospatial data R. library just translates already existing library gdal R. link gdalproject link.raster library use work raster formats.raster library use work raster formats.sf encodes spatial vector data. already used .sf encodes spatial vector data. already used .","code":"require(rgdal)\nrequire(raster) #Package to work with raster files\nrequire(sf) #Package to work with shape files"},{"path":"geo-data.html","id":"load-the-raster-files-and-do-some-calculations","chapter":"4 Geo Data","heading":"4.3.0.1 2. Load the raster files and do some calculations","text":"Let´s suppose different sampling points Germany. interested temperature difference summer winter sites. Can map display info? ?","code":""},{"path":"geo-data.html","id":"this-are-some-usefull-links-to-find-data","chapter":"4 Geo Data","heading":"4.3.0.2 This are some usefull links to find data:","text":"Link Federal Agency Cartography\nLink open BW data\nLink WoldClim\nlink land use data\nAnother useful linkAll data used example download sites directly R.data used example download sites directly R.Raster calculation relatively straight forward using R raster resolution. raster come different sources usually re-shape one raster.info ","code":"t_july<-raster(\"data/wc2.0_5m_tavg_07.tif\") # loads the raster\nt_december<-raster(\"data/wc2.0_5m_tavg_01.tif\") # loads the raster\n# Get temperature range\nt_diff <- abs(t_july - t_december)\nplot(t_diff, main = \"Temperature range\")"},{"path":"geo-data.html","id":"load-the-shape-files","chapter":"4 Geo Data","heading":"4.3.0.3 3. Load the shape files","text":"can see , polling associated data. Geometry info build polygon. level, type, etc. info related polygon. Polygons different info fields, polygons displayed different colors.","code":"sites<-st_read(\"data/sample_sites.shp\") # loads the vector data - sites\n#get germany voundaries from world R data\ngermany <- world %>%  dplyr::filter (name == \"Germany\")\nplot(germany)#get germany voundaries from world R data\ngfs <- world %>%  dplyr::filter (name %in% c(\"Germany\", \"Austria\"))\nplot(gfs)"},{"path":"geo-data.html","id":"check-reference-system","chapter":"4 Geo Data","heading":"4.3.0.4 4. Check reference system","text":"data coming different sources, check coordinate reference system match (otherwise lots problems everything wrong)data coord. reference system. transform one.","code":"# The function to get the coor. system is different between shapes and rasters\nst_crs(germany)#get the projection\nst_crs(sites)#get the projection\ncrs(t_diff)#get the projection\nst_crs(germany)$proj4string == st_crs(sites)$proj4string\nst_crs(germany)$proj4string == crs(t_diff, asText = T)sites_transformed <- st_transform(sites, crs = crs(t_diff, asText = T))#transform coordinate sistem\ngermany_transformed <- st_transform(germany, crs = crs(t_diff, asText = T))#transform coordinate "},{"path":"geo-data.html","id":"crop-the-raster-to-the-shape-file-extent","chapter":"4 Geo Data","heading":"4.3.0.5 5. Crop the raster to the shape file extent","text":"get extent (coordinates extreme points) shape file Germany.order get raster matches exactly shape, mask .\nalso step without cropping first, much computational intensive.","code":"germany_extent <- extent(germany_transformed)\ngermany_extent# crop the land use data by the extend of BW. Crop funciton from the raster package\ncrop_tdiff <- crop(t_diff, germany_extent, snap = \"out\")\nplot(crop_tdiff)mask_tdiff <- mask(crop_tdiff, germany_transformed)\nplot(mask_tdiff)"},{"path":"geo-data.html","id":"plot-with-tmap","chapter":"4 Geo Data","heading":"4.3.0.6 6. Plot with tmap","text":"library tmap another grate option build thematic maps. find useful work raster data. Useful info can found tmap vignette. Another library plot raster data rasterVis. cover today can check vignette\ncan plot many layers together. needs reference(point, raster, text, etc.)tmap really easy interactive maps!tmap great library make maps. Check vignette","code":"require(tmap)\ntmap_mode(\"plot\") # static plot or leaflet. \ntm_shape(mask_tdiff) + \n  tm_raster (col= \"layer\" , style = \"cont\", n=10,  title = \"T. Difference\") +\ntm_shape(germany_transformed) + #add ploygon of germany\n  tm_borders() +\ntm_shape(sites_transformed) +\n   tm_symbols(col = \"gray\", scale = .5)+ ##add points\n      tm_text(\"Site\", size = 0.8,  ymod=-0.6, root=1, size.lowerbound = .60, #add text\n        bg.color=\"gray\", bg.alpha = .5) +\ntm_layout(inner.margins = c(0.15, 0.30, 0.10, 0.1)) + # crop the extent of the map\ntm_layout(legend.position = c(\"left\",\"bottom\")) + # add legent\n    tm_compass() + # add north\n  tm_scale_bar() #add scaletm <- tm_shape(mask_tdiff) + \n  tm_raster (col= \"layer\" , style = \"cont\", n=10,  title = \"T. Difference\") +\ntm_shape(germany_transformed) + #add ploygon of germany\n  tm_borders() +\ntm_shape(sites_transformed) +\n   tm_symbols(col = \"gray\", scale = .5)+ ##add points\n      tm_text(\"Site\", size = 0.8,  ymod=-0.6, root=1, size.lowerbound = .60, #add text\n        bg.color=\"gray\", bg.alpha = .5) +\ntm_layout(inner.margins = c(0.15, 0.30, 0.10, 0.1)) + # crop the extent of the map\ntm_layout(legend.position = c(\"left\",\"bottom\")) + # add legent\n    tm_compass() + # add north\n  tm_scale_bar() #add scale\ntmap_save(tm, filename = \"t_range.png\")require(tmap)\ntmap_mode(\"view\")\ntm_shape(mask_tdiff) +\n  tm_raster (col= \"layer\" , style = \"cont\", n=10,  title = \"T. Difference\") +\ntm_shape(germany_transformed) +\n  tm_borders() +\ntm_shape(sites_transformed) +\n   tm_symbols(col = \"gray\", scale = .5)+\n      tm_text(\"Site\", size = 0.8,  ymod=-0.6, root=1, size.lowerbound = .60, \n        bg.color=\"gray\", bg.alpha = .5) +\ntm_layout(inner.margins = c(0.1, 0.30, 0.10, 0.1)) +\ntm_layout(legend.position = c(\"left\",\"bottom\"))\n##Reclasify values in the rasta flie. Reclasify function of raster file function"},{"path":"regression.html","id":"regression","chapter":"5 Regression","heading":"5 Regression","text":"","code":""},{"path":"regression.html","id":"linear-regression-models","chapter":"5 Regression","heading":"5.1 Linear Regression Models","text":"Linear Regression Linear Model (LM) using happiness data World Health Organization, section brought RLadies Freiburg Divya Elisa. happiness data can found .question , parameters ? questions can address using factors.? trust government variable?","code":"\nplot(happiness_2016$Happiness.Score, happiness_2016$Trust..Government.Corruption.)"},{"path":"regression.html","id":"try-some-models","chapter":"5 Regression","heading":"5.1.1 try some models","text":"3 questions ask:\n- Corruption within government predict happiness citizens?\n- can add Freedom predictor model?\n- difference adding factors looking interactions?number different Parametric non-parametric tests can try format. Instead lm, use t.test(), aov(), wilcox.test(), kruskal.test().categorical variable can use run ANOVA ?happiness quotient differ based regions?","code":"\n## ANSWERS\n#fit our regression model\nRegressionModel <- lm(Happiness.Score ~ Trust..Government.Corruption., # regression formula\n              data=happiness_2016) # data set\n\n# Summarize and print the results\nsummary(RegressionModel) # show regression coefficients table\n#> \n#> Call:\n#> lm(formula = Happiness.Score ~ Trust..Government.Corruption., \n#>     data = happiness_2016)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.3867 -0.7337  0.0653  0.8163  2.0929 \n#> \n#> Coefficients:\n#>                               Estimate Std. Error t value\n#> (Intercept)                     4.8133     0.1335  36.042\n#> Trust..Government.Corruption.   4.1336     0.7562   5.466\n#>                               Pr(>|t|)    \n#> (Intercept)                    < 2e-16 ***\n#> Trust..Government.Corruption.  1.8e-07 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 1.049 on 155 degrees of freedom\n#> Multiple R-squared:  0.1616, Adjusted R-squared:  0.1562 \n#> F-statistic: 29.88 on 1 and 155 DF,  p-value: 1.798e-07\n\n\nRegressionModel2 <- lm(Happiness.Score ~ Trust..Government.Corruption. + Freedom,\n                      data=happiness_2016) \nsummary(RegressionModel2) \n#> \n#> Call:\n#> lm(formula = Happiness.Score ~ Trust..Government.Corruption. + \n#>     Freedom, data = happiness_2016)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -3.12005 -0.75060  0.08456  0.70370  1.99165 \n#> \n#> Coefficients:\n#>                               Estimate Std. Error t value\n#> (Intercept)                     3.7395     0.2047  18.270\n#> Trust..Government.Corruption.   1.6146     0.7785   2.074\n#> Freedom                         3.8288     0.5940   6.445\n#>                               Pr(>|t|)    \n#> (Intercept)                    < 2e-16 ***\n#> Trust..Government.Corruption.   0.0397 *  \n#> Freedom                       1.41e-09 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9337 on 154 degrees of freedom\n#> Multiple R-squared:  0.3397, Adjusted R-squared:  0.3312 \n#> F-statistic: 39.62 on 2 and 154 DF,  p-value: 1.313e-14\n\n\nRegressionModel3 <- lm(Happiness.Score ~ Trust..Government.Corruption. * Freedom, \n                       data=happiness_2016) \n\nsummary(RegressionModel3)\n#> \n#> Call:\n#> lm(formula = Happiness.Score ~ Trust..Government.Corruption. * \n#>     Freedom, data = happiness_2016)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.4328 -0.6808  0.1590  0.6513  2.0290 \n#> \n#> Coefficients:\n#>                                       Estimate Std. Error\n#> (Intercept)                             4.6015     0.3507\n#> Trust..Government.Corruption.          -7.0035     2.9817\n#> Freedom                                 1.8776     0.8728\n#> Trust..Government.Corruption.:Freedom  17.7259     5.9306\n#>                                       t value Pr(>|t|)    \n#> (Intercept)                            13.119  < 2e-16 ***\n#> Trust..Government.Corruption.          -2.349  0.02011 *  \n#> Freedom                                 2.151  0.03303 *  \n#> Trust..Government.Corruption.:Freedom   2.989  0.00326 ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.9105 on 153 degrees of freedom\n#> Multiple R-squared:  0.3762, Adjusted R-squared:  0.3639 \n#> F-statistic: 30.75 on 3 and 153 DF,  p-value: 1.293e-15\nANOVA <- aov(Happiness.Score ~ Region, \n             data=happiness_2016) \nsummary(ANOVA) \n#>              Df Sum Sq Mean Sq F value Pr(>F)    \n#> Region        9 126.69  14.077      27 <2e-16 ***\n#> Residuals   147  76.64   0.521                   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(ANOVA)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = Happiness.Score ~ Region, data = happiness_2016)\n#> \n#> $Region\n#>                                                                    diff\n#> Central and Eastern Europe-Australia and New Zealand        -1.95281034\n#> Eastern Asia-Australia and New Zealand                      -1.69933333\n#> Latin America and Caribbean-Australia and New Zealand       -1.22175000\n#> Middle East and Northern Africa-Australia and New Zealand   -1.93744737\n#> North America-Australia and New Zealand                     -0.06950000\n#> Southeastern Asia-Australia and New Zealand                 -1.98461111\n#> Southern Asia-Australia and New Zealand                     -2.76021429\n#> Sub-Saharan Africa-Australia and New Zealand                -3.18707895\n#> Western Europe-Australia and New Zealand                    -0.63783333\n#> Eastern Asia-Central and Eastern Europe                      0.25347701\n#> Latin America and Caribbean-Central and Eastern Europe       0.73106034\n#> Middle East and Northern Africa-Central and Eastern Europe   0.01536298\n#> North America-Central and Eastern Europe                     1.88331034\n#> Southeastern Asia-Central and Eastern Europe                -0.03180077\n#> Southern Asia-Central and Eastern Europe                    -0.80740394\n#> Sub-Saharan Africa-Central and Eastern Europe               -1.23426860\n#> Western Europe-Central and Eastern Europe                    1.31497701\n#> Latin America and Caribbean-Eastern Asia                     0.47758333\n#> Middle East and Northern Africa-Eastern Asia                -0.23811404\n#> North America-Eastern Asia                                   1.62983333\n#> Southeastern Asia-Eastern Asia                              -0.28527778\n#> Southern Asia-Eastern Asia                                  -1.06088095\n#> Sub-Saharan Africa-Eastern Asia                             -1.48774561\n#> Western Europe-Eastern Asia                                  1.06150000\n#> Middle East and Northern Africa-Latin America and Caribbean -0.71569737\n#> North America-Latin America and Caribbean                    1.15225000\n#> Southeastern Asia-Latin America and Caribbean               -0.76286111\n#> Southern Asia-Latin America and Caribbean                   -1.53846429\n#> Sub-Saharan Africa-Latin America and Caribbean              -1.96532895\n#> Western Europe-Latin America and Caribbean                   0.58391667\n#> North America-Middle East and Northern Africa                1.86794737\n#> Southeastern Asia-Middle East and Northern Africa           -0.04716374\n#> Southern Asia-Middle East and Northern Africa               -0.82276692\n#> Sub-Saharan Africa-Middle East and Northern Africa          -1.24963158\n#> Western Europe-Middle East and Northern Africa               1.29961404\n#> Southeastern Asia-North America                             -1.91511111\n#> Southern Asia-North America                                 -2.69071429\n#> Sub-Saharan Africa-North America                            -3.11757895\n#> Western Europe-North America                                -0.56833333\n#> Southern Asia-Southeastern Asia                             -0.77560317\n#> Sub-Saharan Africa-Southeastern Asia                        -1.20246784\n#> Western Europe-Southeastern Asia                             1.34677778\n#> Sub-Saharan Africa-Southern Asia                            -0.42686466\n#> Western Europe-Southern Asia                                 2.12238095\n#> Western Europe-Sub-Saharan Africa                            2.54924561\n#>                                                                     lwr\n#> Central and Eastern Europe-Australia and New Zealand        -3.64886668\n#> Eastern Asia-Australia and New Zealand                      -3.59354189\n#> Latin America and Caribbean-Australia and New Zealand       -2.92916652\n#> Middle East and Northern Africa-Australia and New Zealand   -3.66205885\n#> North America-Australia and New Zealand                     -2.38942221\n#> Southeastern Asia-Australia and New Zealand                 -3.79817773\n#> Southern Asia-Australia and New Zealand                     -4.62029016\n#> Sub-Saharan Africa-Australia and New Zealand                -4.87012741\n#> Western Europe-Australia and New Zealand                    -2.35460563\n#> Eastern Asia-Central and Eastern Europe                     -0.78700079\n#> Latin America and Caribbean-Central and Eastern Europe       0.09087351\n#> Middle East and Northern Africa-Central and Eastern Europe  -0.66936527\n#> North America-Central and Eastern Europe                     0.18725401\n#> Southeastern Asia-Central and Eastern Europe                -0.91700803\n#> Southern Asia-Central and Eastern Europe                    -1.78436365\n#> Sub-Saharan Africa-Central and Eastern Europe               -1.80630021\n#> Western Europe-Central and Eastern Europe                    0.65024012\n#> Latin America and Caribbean-Eastern Asia                    -0.58131144\n#> Middle East and Northern Africa-Eastern Asia                -1.32451715\n#> North America-Eastern Asia                                  -0.26437522\n#> Southeastern Asia-Eastern Asia                              -1.50798414\n#> Southern Asia-Eastern Asia                                  -2.35156652\n#> Sub-Saharan Africa-Eastern Asia                             -2.50688207\n#> Western Europe-Eastern Asia                                 -0.01241531\n#> Middle East and Northern Africa-Latin America and Caribbean -1.42809953\n#> North America-Latin America and Caribbean                   -0.55516652\n#> Southeastern Asia-Latin America and Caribbean               -1.66964442\n#> Southern Asia-Latin America and Caribbean                   -2.53501551\n#> Sub-Saharan Africa-Latin America and Caribbean              -2.57021260\n#> Western Europe-Latin America and Caribbean                  -0.10929268\n#> North America-Middle East and Northern Africa                0.14333589\n#> Southeastern Asia-Middle East and Northern Africa           -0.98592333\n#> Southern Asia-Middle East and Northern Africa               -1.84849980\n#> Sub-Saharan Africa-Middle East and Northern Africa          -1.90147345\n#> Western Europe-Middle East and Northern Africa               0.56507146\n#> Southeastern Asia-North America                             -3.72867773\n#> Southern Asia-North America                                 -4.55079016\n#> Sub-Saharan Africa-North America                            -4.80062741\n#> Western Europe-North America                                -2.28510563\n#> Southern Asia-Southeastern Asia                             -1.94473408\n#> Sub-Saharan Africa-Southeastern Asia                        -2.06248932\n#> Western Europe-Southeastern Asia                             0.42249865\n#> Sub-Saharan Africa-Southern Asia                            -1.38106345\n#> Western Europe-Southern Asia                                 1.10988389\n#> Western Europe-Sub-Saharan Africa                            1.91843647\n#>                                                                      upr\n#> Central and Eastern Europe-Australia and New Zealand        -0.256754011\n#> Eastern Asia-Australia and New Zealand                       0.194875220\n#> Latin America and Caribbean-Australia and New Zealand        0.485666516\n#> Middle East and Northern Africa-Australia and New Zealand   -0.212835891\n#> North America-Australia and New Zealand                      2.250422211\n#> Southeastern Asia-Australia and New Zealand                 -0.171044494\n#> Southern Asia-Australia and New Zealand                     -0.900138412\n#> Sub-Saharan Africa-Australia and New Zealand                -1.504030481\n#> Western Europe-Australia and New Zealand                     1.078938960\n#> Eastern Asia-Central and Eastern Europe                      1.293954817\n#> Latin America and Caribbean-Central and Eastern Europe       1.371247178\n#> Middle East and Northern Africa-Central and Eastern Europe   0.700091220\n#> North America-Central and Eastern Europe                     3.579366678\n#> Southeastern Asia-Central and Eastern Europe                 0.853406494\n#> Southern Asia-Central and Eastern Europe                     0.169555770\n#> Sub-Saharan Africa-Central and Eastern Europe               -0.662236994\n#> Western Europe-Central and Eastern Europe                    1.979713898\n#> Latin America and Caribbean-Eastern Asia                     1.536478105\n#> Middle East and Northern Africa-Eastern Asia                 0.848289078\n#> North America-Eastern Asia                                   3.524041887\n#> Southeastern Asia-Eastern Asia                               0.937428586\n#> Southern Asia-Eastern Asia                                   0.229804615\n#> Sub-Saharan Africa-Eastern Asia                             -0.468609157\n#> Western Europe-Eastern Asia                                  2.135415306\n#> Middle East and Northern Africa-Latin America and Caribbean -0.003295206\n#> North America-Latin America and Caribbean                    2.859666516\n#> Southeastern Asia-Latin America and Caribbean                0.143922197\n#> Southern Asia-Latin America and Caribbean                   -0.541913057\n#> Sub-Saharan Africa-Latin America and Caribbean              -1.360445294\n#> Western Europe-Latin America and Caribbean                   1.277126016\n#> North America-Middle East and Northern Africa                3.592558845\n#> Southeastern Asia-Middle East and Northern Africa            0.891595840\n#> Southern Asia-Middle East and Northern Africa                0.202965961\n#> Sub-Saharan Africa-Middle East and Northern Africa          -0.597789711\n#> Western Europe-Middle East and Northern Africa               2.034156606\n#> Southeastern Asia-North America                             -0.101544494\n#> Southern Asia-North America                                 -0.830638412\n#> Sub-Saharan Africa-North America                            -1.434530481\n#> Western Europe-North America                                 1.148438960\n#> Southern Asia-Southeastern Asia                              0.393527727\n#> Sub-Saharan Africa-Southeastern Asia                        -0.342446355\n#> Western Europe-Southeastern Asia                             2.271056910\n#> Sub-Saharan Africa-Southern Asia                             0.527334128\n#> Western Europe-Southern Asia                                 3.134878013\n#> Western Europe-Sub-Saharan Africa                            3.180054762\n#>                                                                 p adj\n#> Central and Eastern Europe-Australia and New Zealand        0.0110279\n#> Eastern Asia-Australia and New Zealand                      0.1200475\n#> Latin America and Caribbean-Australia and New Zealand       0.3959950\n#> Middle East and Northern Africa-Australia and New Zealand   0.0148505\n#> North America-Australia and New Zealand                     1.0000000\n#> Southeastern Asia-Australia and New Zealand                 0.0200680\n#> Southern Asia-Australia and New Zealand                     0.0001878\n#> Sub-Saharan Africa-Australia and New Zealand                0.0000004\n#> Western Europe-Australia and New Zealand                    0.9722986\n#> Eastern Asia-Central and Eastern Europe                     0.9987321\n#> Latin America and Caribbean-Central and Eastern Europe      0.0122030\n#> Middle East and Northern Africa-Central and Eastern Europe  1.0000000\n#> North America-Central and Eastern Europe                    0.0170083\n#> Southeastern Asia-Central and Eastern Europe                1.0000000\n#> Southern Asia-Central and Eastern Europe                    0.2021961\n#> Sub-Saharan Africa-Central and Eastern Europe               0.0000000\n#> Western Europe-Central and Eastern Europe                   0.0000001\n#> Latin America and Caribbean-Eastern Asia                    0.9092820\n#> Middle East and Northern Africa-Eastern Asia                0.9994553\n#> North America-Eastern Asia                                  0.1587204\n#> Southeastern Asia-Eastern Asia                              0.9990999\n#> Southern Asia-Eastern Asia                                  0.2085183\n#> Sub-Saharan Africa-Eastern Asia                             0.0002599\n#> Western Europe-Eastern Asia                                 0.0555311\n#> Middle East and Northern Africa-Latin America and Caribbean 0.0479233\n#> North America-Latin America and Caribbean                   0.4832340\n#> Southeastern Asia-Latin America and Caribbean               0.1822793\n#> Southern Asia-Latin America and Caribbean                   0.0000824\n#> Sub-Saharan Africa-Latin America and Caribbean              0.0000000\n#> Western Europe-Latin America and Caribbean                  0.1809182\n#> North America-Middle East and Northern Africa               0.0224810\n#> Southeastern Asia-Middle East and Northern Africa           1.0000000\n#> Southern Asia-Middle East and Northern Africa               0.2380242\n#> Sub-Saharan Africa-Middle East and Northern Africa          0.0000003\n#> Western Europe-Middle East and Northern Africa              0.0000030\n#> Southeastern Asia-North America                             0.0294290\n#> Southern Asia-North America                                 0.0003101\n#> Sub-Saharan Africa-North America                            0.0000008\n#> Western Europe-North America                                0.9873720\n#> Southern Asia-Southeastern Asia                             0.5086390\n#> Sub-Saharan Africa-Southeastern Asia                        0.0005847\n#> Western Europe-Southeastern Asia                            0.0002694\n#> Sub-Saharan Africa-Southern Asia                            0.9133646\n#> Western Europe-Southern Asia                                0.0000000\n#> Western Europe-Sub-Saharan Africa                           0.0000000"},{"path":"regression.html","id":"independence-of-the-data","chapter":"5 Regression","heading":"5.1.2 independence of the data","text":"Let’s think data pose questions.think countries region tend similar ?\nanswer yes, countries really independent identically distributed data. problem statistical models. three options:Ignore problem, argue think data independent (one)\nIgnore problem, argue think data independent (one)ignore , take average region make model using average. Yes, think correct, loose lot data!\nignore , take average region make model using average. Yes, think correct, loose lot data!one linear model region…data costly (e.g. Australia North America two data points)\none linear model region…data costly (e.g. Australia North America two data points)Try linear mixed model\nTry linear mixed modelHow variable happiness regions?variable happiness regions?happiness depend predictors health, economy generosity?happiness depend predictors health, economy generosity?","code":""},{"path":"regression.html","id":"visualization-of-the-data","chapter":"5 Regression","heading":"5.1.3 visualization of the data","text":"can plots see data look like.\nplot visualize economy affects happiness every region. one plot per predictor.Given see data, can try two different models:p-values hummm!","code":"\nlibrary(dplyr)\n#> \n#> Attaching package: 'dplyr'\n#> The following objects are masked from 'package:stats':\n#> \n#>     filter, lag\n#> The following objects are masked from 'package:base':\n#> \n#>     intersect, setdiff, setequal, union\nlibrary(tidyr)\nHappiness_2016 <- happiness_2016 %>% group_by(Region) %>% mutate(mean.reg = mean(Happiness.Score)) %>%  ungroup()\nlibrary(ggplot2)\nggplot(Happiness_2016) + \n  aes(x = Economy..GDP.per.Capita., y = Happiness.Score) + \n  stat_smooth(method = \"lm\", se = FALSE) +\n  # Put the points on top of lines\n  geom_point() +\n  facet_wrap(\"Region\") +\n  labs(x = \"Economy\", y = \"Happyness\") +\n  geom_hline(aes(yintercept = mean.reg), colour='red', lty=\"dotted\")\n#> `geom_smooth()` using formula 'y ~ x'\nlibrary(lme4)\n#> Loading required package: Matrix\n#> \n#> Attaching package: 'Matrix'\n#> The following objects are masked from 'package:tidyr':\n#> \n#>     expand, pack, unpack\nhappy.mixed.model <-  lmer(Happiness.Score ~ Economy..GDP.per.Capita. + Health..Life.Expectancy.+ Generosity + (1|Region), data = happiness_2016)\n\nsummary(happy.mixed.model)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: \n#> Happiness.Score ~ Economy..GDP.per.Capita. + Health..Life.Expectancy. +  \n#>     Generosity + (1 | Region)\n#>    Data: happiness_2016\n#> \n#> REML criterion at convergence: 288.4\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.7228 -0.5314  0.0542  0.6596  3.6518 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Region   (Intercept) 0.1295   0.3599  \n#>  Residual             0.3302   0.5746  \n#> Number of obs: 157, groups:  Region, 10\n#> \n#> Fixed effects:\n#>                          Estimate Std. Error t value\n#> (Intercept)                3.0080     0.2890  10.407\n#> Economy..GDP.per.Capita.   1.5252     0.2136   7.142\n#> Health..Life.Expectancy.   1.1030     0.4661   2.367\n#> Generosity                 1.2551     0.4088   3.070\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) E..GDP H..L.E\n#> Ec..GDP..C. -0.217              \n#> Hlth..Lf.E. -0.468 -0.624       \n#> Generosity  -0.415  0.174 -0.118\n\nhappy.mixed.model.2 <-  lmer(Happiness.Score ~ Economy..GDP.per.Capita. + Health..Life.Expectancy.+ Generosity + (Economy..GDP.per.Capita.|Region), \n                             data = happiness_2016)\n\nsummary(happy.mixed.model)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: \n#> Happiness.Score ~ Economy..GDP.per.Capita. + Health..Life.Expectancy. +  \n#>     Generosity + (1 | Region)\n#>    Data: happiness_2016\n#> \n#> REML criterion at convergence: 288.4\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.7228 -0.5314  0.0542  0.6596  3.6518 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Region   (Intercept) 0.1295   0.3599  \n#>  Residual             0.3302   0.5746  \n#> Number of obs: 157, groups:  Region, 10\n#> \n#> Fixed effects:\n#>                          Estimate Std. Error t value\n#> (Intercept)                3.0080     0.2890  10.407\n#> Economy..GDP.per.Capita.   1.5252     0.2136   7.142\n#> Health..Life.Expectancy.   1.1030     0.4661   2.367\n#> Generosity                 1.2551     0.4088   3.070\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) E..GDP H..L.E\n#> Ec..GDP..C. -0.217              \n#> Hlth..Lf.E. -0.468 -0.624       \n#> Generosity  -0.415  0.174 -0.118\nsummary(happy.mixed.model.2)\n#> Linear mixed model fit by REML ['lmerMod']\n#> Formula: \n#> Happiness.Score ~ Economy..GDP.per.Capita. + Health..Life.Expectancy. +  \n#>     Generosity + (Economy..GDP.per.Capita. | Region)\n#>    Data: happiness_2016\n#> \n#> REML criterion at convergence: 276.9\n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -3.7297 -0.5007  0.0021  0.6491  3.1793 \n#> \n#> Random effects:\n#>  Groups   Name                     Variance Std.Dev. Corr \n#>  Region   (Intercept)              0.09681  0.3111        \n#>           Economy..GDP.per.Capita. 0.42078  0.6487   -0.87\n#>  Residual                          0.29387  0.5421        \n#> Number of obs: 157, groups:  Region, 10\n#> \n#> Fixed effects:\n#>                          Estimate Std. Error t value\n#> (Intercept)                2.9115     0.2270  12.824\n#> Economy..GDP.per.Capita.   1.8639     0.3101   6.010\n#> Health..Life.Expectancy.   0.6234     0.4415   1.412\n#> Generosity                 1.0923     0.3895   2.805\n#> \n#> Correlation of Fixed Effects:\n#>             (Intr) E..GDP H..L.E\n#> Ec..GDP..C. -0.390              \n#> Hlth..Lf.E. -0.323 -0.559       \n#> Generosity  -0.440  0.049 -0.092\nlibrary(lmerTest)\n#> \n#> Attaching package: 'lmerTest'\n#> The following object is masked from 'package:lme4':\n#> \n#>     lmer\n#> The following object is masked from 'package:stats':\n#> \n#>     step\n# Test significance of random effects\nranova(happy.mixed.model.2)\n#> ANOVA-like table for random-effects: Single term deletions\n#> \n#> Model:\n#> Happiness.Score ~ Economy..GDP.per.Capita. + Health..Life.Expectancy. + Generosity + (Economy..GDP.per.Capita. | Region)\n#>                                                                 npar\n#> <none>                                                             8\n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region)    6\n#>                                                                  logLik\n#> <none>                                                          -138.45\n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region) -144.21\n#>                                                                    AIC\n#> <none>                                                          292.89\n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region) 300.42\n#>                                                                    LRT\n#> <none>                                                                \n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region) 11.529\n#>                                                                 Df\n#> <none>                                                            \n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region)  2\n#>                                                                 Pr(>Chisq)\n#> <none>                                                                    \n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region)   0.003137\n#>                                                                   \n#> <none>                                                            \n#> Economy..GDP.per.Capita. in (Economy..GDP.per.Capita. | Region) **\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n#Test significance of fixed effects\nml.happy.mixed.model <- update(happy.mixed.model, REML = FALSE) # this changes the algorithm used to fit the model\n\n#Finally we can test significance\nanova(as_lmerModLmerTest(ml.happy.mixed.model))\n#> Type III Analysis of Variance Table with Satterthwaite's method\n#>                           Sum Sq Mean Sq NumDF  DenDF\n#> Economy..GDP.per.Capita. 16.9971 16.9971     1 152.49\n#> Health..Life.Expectancy.  1.9558  1.9558     1 138.78\n#> Generosity                3.1992  3.1992     1 151.95\n#>                          F value    Pr(>F)    \n#> Economy..GDP.per.Capita. 52.3454 2.119e-11 ***\n#> Health..Life.Expectancy.  6.0233  0.015356 *  \n#> Generosity                9.8524  0.002038 ** \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"regression.html","id":"ploting-the-data-in-a-map","chapter":"5 Regression","heading":"5.1.4 Ploting the data in a map","text":"","code":"\nrequire(rnaturalearth)\n#> Loading required package: rnaturalearth\nrequire(rnaturalearthdata)\n#> Loading required package: rnaturalearthdata\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\") # this is another function to get polygons of countries. \nlibrary(ggplot2)\nggplot(data = world) +\n    theme_bw()+ \n    geom_sf() + \n    xlab(\"Longitude\") + ylab(\"Latitude\") + \n    ggtitle(\"World map\", subtitle = paste0(\"(\", dim(world)[1], \" countries)\"))#both df woth the same name of the varaible we will use to join\ncolnames(happiness_2016)[1] <- \"name\"\nHappiness_GEO <- left_join(world, happiness_2016, by=\"name\")ggplot(data = Happiness_GEO) +\n    geom_sf(aes(fill = Happiness.Score )) +\n    scale_fill_viridis_c(option = \"plasma\") +  # this allows you to choose different colour scale\n    ggtitle(\"World Happiness Studies\")"},{"path":"regression.html","id":"where-is-the-model-working-better","chapter":"5 Regression","heading":"5.1.4.1 Where is the model working better?","text":"","code":"happy.predictions <- predict(happy.mixed.model)\n#Expected - Observed\nhappy.residuasl <-  (happiness_2016$Happiness.Score - happy.predictions) # Obaserved - predicted\nHappiness_2016$Model.Residuals <- happy.residuasl\nHappiness_GEO <- left_join(world, Happiness_2016, by=\"name\")ggplot(data = Happiness_GEO) +\n    geom_sf(aes(fill = Model.Residuals )) +\n    scale_fill_viridis_c(option = \"plasma\") +  # this allows you to choose different colour scale\n    ggtitle(\"World Happiness Studies\")"},{"path":"text-analysis.html","id":"text-analysis","chapter":"6 Text Analysis","heading":"6 Text Analysis","text":"","code":""},{"path":"text-analysis.html","id":"text-mining","chapter":"6 Text Analysis","heading":"6.1 Text Mining","text":"section RLadies Freiburg Kyla McConnell & Julia Müller. source material Text Mining R Julia Silge & David Robson.libraries needed tutorial","code":"\nlibrary(tidyverse)  # for various data manipulation tasks\n#> ── Attaching packages ─────────────────── tidyverse 1.3.1 ──\n#> ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n#> ✓ tibble  3.1.6     ✓ dplyr   1.0.7\n#> ✓ tidyr   1.1.4     ✓ stringr 1.4.0\n#> ✓ readr   2.1.1     ✓ forcats 0.5.1\n#> ── Conflicts ────────────────────── tidyverse_conflicts() ──\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()\nlibrary(tidytext)   # for text mining specifically, main package in book\nlibrary(stringr)    # for various text operations\nlibrary(gutenbergr) # to access full-text books that are in the public domain\nlibrary(scales)     # for visualizing percentages\n#> \n#> Attaching package: 'scales'\n#> The following object is masked from 'package:purrr':\n#> \n#>     discard\n#> The following object is masked from 'package:readr':\n#> \n#>     col_factor\nlibrary(readtext)   # for reading in txt files\nlibrary(wordcloud)  # for creating wordclouds\n#> Loading required package: RColorBrewer"},{"path":"text-analysis.html","id":"reading-in-texts","chapter":"6 Text Analysis","heading":"6.1.1 Reading in texts","text":"’s can read one .txt file saved location script (.e. folder computer):want read files sub-folder, type name folder followed / * ask R read files folder:","code":"hf <- readtext(\"Adventures of Huckleberry Finn.txt\")shakes <- readtext(\"Shakespeare txts/*\")\nshakes$doc_id <- sub(\".txt\", \"\", shakes$doc_id) # this gets rid of .txt in the play titles"},{"path":"text-analysis.html","id":"book-data-from-project-gutenberg","chapter":"6 Text Analysis","heading":"6.1.2 Book data from Project Gutenberg","text":"Project Gutenberg: free downloads books public domain (.e. lots classic literature)Currently legal battle Germany -* impossible download via websiteStill accessible via R package gutenbergr IDTop 100 books inspiration (changes daily based demand): https://www.gutenberg.org/browse/scores/topCatalog: https://www.gutenberg.org/catalog/find id book (multiple copies):Can also search author name:Gutenberg search options: https://ropensci.org/tutorials/gutenbergr_tutorial/found books, download gutenberg_download:11: Alice’s Adventures Wonderland\n2591: Grimm’s Fairytales\n1597: Hans Christian Anderson’s Fairytales","code":"gutenberg_metadata %>%\n  filter(title %in% c(\"Alice's Adventures in Wonderland\", \"Grimms' Fairy Tales\", \"Andersen's Fairy Tales\"))gutenberg_works(author == \"Carroll, Lewis\")\ngutenberg_works(str_detect(author, \"Carroll\")) #if you only have a partial namefairytales_raw <- gutenberg_download(c(11, 2591, 1597))\nfairytales_raw"},{"path":"text-analysis.html","id":"preparing-data","chapter":"6 Text Analysis","heading":"6.1.3 Preparing data","text":"convert Gutenberg ID factor replacing ID numbers descriptive labels","code":"fairytales_raw$gutenberg_id <- as.factor(fairytales_raw$gutenberg_id)\nfairytales_raw$gutenberg_id <- plyr::revalue(fairytales_raw$gutenberg_id,\n                                              c(\"11\" = \"Alice's Adventures in Wonderland\",\n                                                \"2591\" = \"Grimm's Fairytales\",\n                                                \"1597\" = \"Hans Christian Anderson's Fairytales\"))"},{"path":"text-analysis.html","id":"tidy-text","chapter":"6 Text Analysis","heading":"6.2 Tidy text","text":"One word per row, facilitates analysisToken: “meaningful unit text, often word, interested using analysis”","code":""},{"path":"text-analysis.html","id":"the-unnest_tokens-function","chapter":"6 Text Analysis","heading":"6.2.1 the unnest_tokens function","text":"Easy convert full text token per row unnest_tokens()\nSyntax: unnest_tokens(df, newcol, oldcol)unnest_tokens() automatically removes punctuation converts lowercase (unless set to_lower = FALSE)default, tokens set words, can also use token = “characters,” “ngrams,” “sentences,” “lines,” “regex,” “paragraphs,” even “tweets” (retain usernames, hashtags, URLs)","code":"fairytales_tidy <- fairytales_raw %>% \n  unnest_tokens(word, text)\n# this keeps the information on which sentence the words came from\nfairytales_raw %>% \n  unnest_tokens(sentence, text, token = \"sentences\") %>% \n  mutate(sent_nr = row_number()) %>% \n  unnest_tokens(word, sentence)\nfairytales_tidy\nshakes_unnest <- shakes %>% \n  unnest_tokens(word, text)"},{"path":"text-analysis.html","id":"removing-non-alphanumeric-characters","chapter":"6 Text Analysis","heading":"6.2.2 Removing non-alphanumeric characters","text":"Project Gutenberg data sometimes contains underscores indicate italicsstr_extract used get rid non-alphanumeric characters (don’t want count word separately word)","code":"fairytales_tidy <- fairytales_tidy %>% \n  mutate(word = str_extract(word, \"[a-z']+\"))\nshakes_unnest <- shakes_unnest %>% \n  mutate(word = str_extract(word, \"[a-z']+\"))"},{"path":"text-analysis.html","id":"stop-words","chapter":"6 Text Analysis","heading":"6.2.3 Stop words","text":"Stop words: common, “meaningless” function words like “,” “” “” – usually important analysis (.e. find common word two books comparing “”)tidytext built-df called stop_words Englishremove dataset anti_joinWe can take look:Define stop words:also used remove character names, example.stopwords package also contains lists stopwords languages, get list German stopwords, use:info: https://cran.r-project.org/web/packages/stopwords/readme/README.htmlBreak: Prepare data steps . 1) Unnest tokens, 2) Remove alpha-numeric characters, 3) Remove stopwords","code":"stop_wordsfairytales_tidy <- fairytales_tidy %>% \n  anti_join(stop_words)\nfairytales_tidymeaningless_words <- tibble(word = c(\"von\", \"der\", \"thy\", \"thee\", \"thou\"))\nfairytales_tidy <- fairytales_tidy %>% \n  anti_join(meaningless_words)library(stopwords)\nstop_german <- data.frame(word = stopwords::stopwords(\"de\"), stringsAsFactors = FALSE)"},{"path":"text-analysis.html","id":"analysing-frequencies","chapter":"6 Text Analysis","heading":"6.3 Analysing frequencies","text":"","code":""},{"path":"text-analysis.html","id":"find-most-frequent-words","chapter":"6 Text Analysis","heading":"6.3.1 Find most frequent words","text":"Easily find frequent words using count()Data must tidy format (one token per line)sort = TRUE show frequent words firsttidy_books %>%\ncount(word, sort = TRUE)Reminder: filter can used look subsets data, .e. one book, words freq 100, etc. (Note don’t save output)","code":"fairytales_freq <- fairytales_tidy %>% \n  group_by(gutenberg_id) %>% #including this ensures that the counts are by book and the id column is retained\n  count(word, sort=TRUE)\nfairytales_freq\nshakes_freq <- shakes_unnest %>% \n  group_by(doc_id) %>% \n  count(word, sort = TRUE)fairytales_tidy %>% \n  group_by(gutenberg_id) %>% \n  count(word, sort=TRUE) %>% \n  filter(gutenberg_id == \"Grimm's Fairytales\")"},{"path":"text-analysis.html","id":"plotting-word-frequencies---bar-graphs","chapter":"6 Text Analysis","heading":"6.3.1.1 Plotting word frequencies - bar graphs","text":"Bar graph top words Grimm’s fairytales.Basic graph:Readable labels:Descending order:Axis names colors:: flip coordinate system make space words","code":"fairytales_freq %>% \n  filter(n>90 & gutenberg_id == \"Grimm's Fairytales\") %>% \n  ggplot(aes(x=word, y=n)) +\n  geom_col()fairytales_freq %>% \n  filter(n>90 & gutenberg_id == \"Grimm's Fairytales\") %>% \n  ggplot(aes(x=word, y=n)) +\n  geom_col() +\n  theme(axis.text.x = element_text(angle = 45))fairytales_freq %>% \n  filter(n>90 & gutenberg_id == \"Grimm's Fairytales\") %>% \n  ggplot(aes(x=reorder(word, -n), y=n)) +\n  geom_col() +\n  theme(axis.text.x = element_text(angle = 45))fairytales_freq %>% \n  filter(n>90 & gutenberg_id == \"Grimm's Fairytales\") %>% \n  ggplot(aes(x=reorder(word, -n), y=n, fill=n)) +\n  geom_col(show.legend=FALSE) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  xlab(\"Word\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Most frequent words in Grimm's Fairytales\")fairytales_freq %>% \n  filter(n>90 & gutenberg_id == \"Grimm's Fairytales\") %>% \n  ggplot(aes(x=reorder(word, n), y=n, fill=n)) +\n  geom_col(show.legend=FALSE) +\n  xlab(\"Word\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Most frequent words in Grimm's Fairytales\") +\n  coord_flip()"},{"path":"text-analysis.html","id":"normalised-frequency","chapter":"6 Text Analysis","heading":"6.3.2 Normalised frequency","text":"comparing frequencies words different texts, commonly normalisedconvention corpus linguistics: report frequency per 1 million wordsfor shorter texts: per 10,000 per 100,000 wordscalculation: raw frequency * 1,000,000 / total numbers text","code":"# see the total number of words per play (doc_id)\nshakes_freq %>% \n  group_by(doc_id) %>% \n  mutate(sum(n)) %>% \n  distinct(doc_id, sum(n))\nshakes_freq <- shakes_freq %>% \n  na.omit() %>% \n  group_by(doc_id) %>% \n  mutate(pmw = n*1000000/sum(n)) %>% # creates a new column called pmw\n  ungroup() %>% \n  anti_join(stop_words) # removing stopwords afterwards\nshakes_freq %>% select(word, pmw)"},{"path":"text-analysis.html","id":"plotting-normalised-frequency","chapter":"6 Text Analysis","heading":"6.3.2.1 Plotting normalised frequency","text":"Now can plot, example, 20 frequent words (pmw).Break: Gather frequency counts text, normalize , create bar chart normalized frequencies.","code":"shakes_freq %>% \n  filter(doc_id == \"Othello\") %>% \n  top_n(20, pmw) %>% \n  ggplot(aes(x=reorder(word, -pmw), y=pmw, fill=pmw)) +\n  geom_col(show.legend=FALSE) +\n  theme(axis.text.x = element_text(angle = 45)) +\n  xlab(\"Word\") +\n  ylab(\"Frequency per 1 million words\") +\n  ggtitle(\"Most frequent words in Othello\")"},{"path":"text-analysis.html","id":"word-clouds","chapter":"6 Text Analysis","heading":"6.3.3 Word clouds","text":"Let’s visualise frequent words word cloud. , size indicates frequency, words occur often displayed larger font size, can also used visualise e.g. normalised frequency (pmw) length anything else pass freq = part command.","code":"wordcloud(words = shakes_freq$word, freq = shakes_freq$n, \n          min.freq = 150, max.words=200, random.order=FALSE, rot.per=0.35, \n          colors=brewer.pal(8, \"Dark2\"))"},{"path":"text-analysis.html","id":"comparing-the-vocabulary-of-texts","chapter":"6 Text Analysis","heading":"6.4 Comparing the vocabulary of texts","text":"Next, ’ll create two graphs compare vocabulary texts. First, focus Alice’s Adventures Anderson’s Fairytales. newly created comp_2 data frame contains words frequencies two texts two separate columns.","code":""},{"path":"text-analysis.html","id":"comparing-two-texts","chapter":"6 Text Analysis","heading":"6.4.1 Comparing two texts","text":"Now, can plot words. placement depends word frequencies. Additionally, colour coding shows different frequencies - darker items similar terms frequencies, lighter-coloured ones frequent one text compared . ’ll discuss interpretation detail ’ve created threeway comparison.","code":"comp_2 <- fairytales_freq %>% \n  filter(gutenberg_id == \"Alice's Adventures in Wonderland\"|gutenberg_id == \"Hans Christian Anderson's Fairytales\") %>% \n  group_by(gutenberg_id) %>% \n  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)\n  select(-n) %>%\n  spread(gutenberg_id, proportion)\nhead(comp_2)ggplot(comp_2, \n       aes(x = `Alice's Adventures in Wonderland`, y = `Hans Christian Anderson's Fairytales`, \n           color = abs(`Alice's Adventures in Wonderland` - `Hans Christian Anderson's Fairytales`))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  theme_light() +\n  theme(legend.position=\"none\") +\n  labs(y = \"Hans Christian Anderson's Fairytales\", x = \"Alice's Adventures in Wonderland\")"},{"path":"text-analysis.html","id":"comparing-three-texts","chapter":"6 Text Analysis","heading":"6.4.2 Comparing three texts","text":"order compare three texts, need add one step data preparation: Grimm’s Fairytales text two compared , word frequencies contained Grimm’s Fairytales column. gutenberg_id column contains Alice’s Adventures Anderson’s Fairytales can pass column facet_wrap command create two graphs.ggplot command similar one used facet_wrap added create two comparisons - Grimm’s Fairytales compared Alice’s Adventures (left graph) Grimm’s compared Anderson’s fairytales (right graph):","code":"comp_3 <- fairytales_freq %>% \n  group_by(gutenberg_id) %>% \n  mutate(proportion = n / sum(n)) %>% #creates proportion column (word frequency divided by overall frequency per author)\n  select(-n) %>% \n  spread(gutenberg_id, proportion) %>% \n  gather(gutenberg_id, proportion, \"Alice's Adventures in Wonderland\":\"Hans Christian Anderson's Fairytales\") # only done for plotting\nhead(comp_3); unique(comp_3$gutenberg_id)ggplot(comp_3, \n       aes(x = proportion, y = `Grimm's Fairytales`, \n           color = abs(`Grimm's Fairytales` - proportion))) +\n  geom_abline(color = \"gray40\", lty = 2) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +\n  geom_text(aes(label = word), check_overlap = TRUE) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  theme_light() +\n  facet_wrap(~ gutenberg_id, ncol = 2) +\n  theme(legend.position=\"none\") +\n  labs(y = \"Grimm's Fairytales\", x = NULL)"},{"path":"text-analysis.html","id":"interpretation","chapter":"6 Text Analysis","heading":"6.4.3 Interpretation","text":"Words close diagonal line similar frequencies texts (e.g. king, cook, calling left graph). Words far line frequent one two texts (e.g. wife son frequent Grimm compared Alice, turtle hare frequent Alice Grimm). , also indicated colour.\nGenerally, words closer line ’s smaller gap low frequencies, vocabulary texts overall similar. case, two fairytale sources contain words Grimm compared Alice.\nadditional step analysis word frequencies something won’t cover today calculate correlations word frequencies quantify similar vocabularies texts .","code":""},{"path":"text-analysis.html","id":"word-and-document-frequencies","chapter":"6 Text Analysis","heading":"6.5 Word and document frequencies","text":"","code":""},{"path":"text-analysis.html","id":"tf-idf","chapter":"6 Text Analysis","heading":"6.5.1 tf-idf","text":"can quantify text/document ? analyse term frequency (tf) - often term occur text/document. However, common words texts, e.g. grammatical words like articles. solution instead analyse inverse document frequency (idf) lowers importance frequent words raises importance rare words documents. ’s measure important word text compared important collection texts.","code":""},{"path":"text-analysis.html","id":"the-bind_tf_idf-function","chapter":"6 Text Analysis","heading":"6.5.1.1 The bind_tf_idf-function","text":"input: format needs one row per token (term), per documentone column (: word) contains terms/tokensone column (: gutenberg_id) contains documentsinterpretation:low tf_idf words appear many books, high occur bookscharacteristic words documentsso unsurprisingly, data, first hits highest tf_idf character names","code":"fairytales_idf <- fairytales_freq %>% \n  bind_tf_idf(word, gutenberg_id, n)\nfairytales_idf %>%\n  select(gutenberg_id, word, tf_idf) %>% \n  arrange(desc(tf_idf))"},{"path":"text-analysis.html","id":"characteristic-words-per-book","chapter":"6 Text Analysis","heading":"6.5.1.2 Characteristic words per book","text":"visualisation top 20 tf-idf words per book:","code":"fairytales_idf$word <- as.factor(fairytales_idf$word)\nfairytales_idf %>%\n  group_by(gutenberg_id) %>% \n  arrange(desc(tf_idf)) %>% \n  top_n(20, tf_idf) %>% \n  ggplot(aes(x = reorder(word, tf_idf), y = tf_idf, fill = gutenberg_id)) +\n  geom_col(show.legend = FALSE) +\n  labs(x = NULL, y = \"tf-idf\") +\n  facet_wrap(~gutenberg_id, scales = \"free\") +\n  coord_flip()"},{"path":"text-analysis.html","id":"characteristic-words-per-chapter","chapter":"6 Text Analysis","heading":"6.5.1.3 Characteristic words per chapter","text":"tf_idf can also used find characteristic words per chapter, text unit. ’ll use “Alice Wonderland” example since consists several chapters.\nfirst need create column contains chapter word came . code :finds “chapter” followed Roman numeral using regular expression regex(“^chapter [\\divclx]” str_detect() commandextracts chapter number counting often regex found cumsum(). basically counter starts 0 regex isn’t matched, counts one every time chapter + Roman numeral found textwrite new column called “chapter”also preserves original line numbers (optional)\nremove gutenberg_id column, words chapter 0, .e. title information edition, unnest tokens, remove stopwords.Now let’s calculate tf-idf per chapter:first step calculate frequency word per chapterthen apply bind_tf_idf functionshow words highest tf_idfAgain, can visualise characteristic words, time per chapter:","code":"alice <- fairytales_raw %>% \n  filter(gutenberg_id == \"Alice's Adventures in Wonderland\") %>% \n  mutate(linenumber = row_number(),\n         chapter = cumsum(str_detect(text, regex(\"^chapter [\\\\divclx]\",\n                                                 ignore_case = TRUE)))) %>%\n  select(-gutenberg_id) %>% \n  filter(chapter != 0) %>% \n  mutate(chapter = as_factor(chapter)) %>% \n  unnest_tokens(word, text) %>% \n  anti_join(stop_words)\nhead(alice); summary(alice$chapter)alice <- alice %>% \n  group_by(chapter) %>% \n  count(word, sort = TRUE)\nalice_idf <- alice %>% \n  bind_tf_idf(word, chapter, n)\nalice_idf %>%\n  select(chapter, word, tf_idf) %>% \n  arrange(desc(tf_idf))alice_idf %>%\n  group_by(chapter) %>% \n  arrange(desc(tf_idf)) %>% \n  top_n(5, tf_idf) %>% \n  ggplot(aes(reorder(word, tf_idf), tf_idf, fill = chapter)) +\n  geom_col(show.legend = FALSE) +\n  theme_light() +\n  labs(x = NULL, y = \"tf-idf\") +\n  facet_wrap(~ chapter, scales = \"free\") +\n  coord_flip()"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
